//Gotta put this in gitignore

### Why does ebpf need us to lock the RAM?

- In Linux kernel, certain paths are critical paths that the kernel treats as high priority.
- We are hooking into these criticial paths (like recieving a packet)
- So when the kernel waits for our code to be swapped back in from the SSD the entire device hangs
- Wow. 

### Where did monitorObjects come from?

- On running `go generate ./...`, a tool called bpf2go reads the C code and automatically writes a new file called monitor_bpfel.go.
- Inside this file the tool creates a struct called monitorObjects.
- Because this file is in the same package name, we can directly access it.

### What is monitorObjects?

- a struct
- Our C code in this case has a function (handle_packet) and a map (my_ringbuf), then the struct stores one container for C functions and one for BPF Maps/Ringbufs
- 
```
type monitorObjects struct {
    MonitorPrograms // Container for your C functions
    MonitorMaps     // Container for your BPF Maps/Ringbufs
}
```
### What is loadMonitorObjects?

- Also generated by bpf2go
- Does three main things-
1. Loading: the C bytecode is pushed to the kernel which then runs the Verifier.
2. Allocates space for Maps that the C code might have defined in the RAM and locks the memory for it.
3. Wiring: The function takes the pointers (File Descriptors) to the programs and maps that the kernel just created and "plugs" them into our objs struct.
- After this function runs, our objs.MyRingBuf is no longer empty; it is now a live "pipe" connected directly to the kernel's memory.

### Why are we even loading RawBytes from C into Go when Go is essentially just logging packets being dropped and doesn't need to compile and send things to the kernel?

- Portability: prevents us form having to carry .o files around, just compile the one Go file on any OS/device and boom.
- Kernel Translator: If we compile the C code on my laptop and move the raw binary to a server, it will crash or report garbage data because the memory addresses won't match. 
- bf2go fixes this by looking at the specific kernel running on the machine rn, reading it's BTF (BPF Type Format) data from that kernel (the kernel's own map of its structures), and rewrites the C bytecode on the fly to point to the correct memory offsets before pushing it into the kernel.

### How does the C to Go flow work?

- We write the logic in C which has instructions for the kernel
- Running `go generate` triggers a compiler (Clang) which turns the C code into eBPF Bytecode.
- This Bytecode is stored in an ELF object file (monitor_bpfle.o in our case).
- The Bytecode is embedded into a Go variable: bpf2go reads the .o file and copy pastes the binary data into a brand new Go soruce file as a long slice of bytes. (`monitorBytes` in monitor_bpfle.go here).
- Now upon `go build` this bytecode will be built into our final Go executable.
- Then `main.go` starts and calls loadMonitorObjects() which takes monitorBytes slice and hands it to Linux Kernel via bpf() system call. The Verifier runs. 
- Kernel's JIT (Just-In-Time) Compiler converts the generic eBPF code into specific machine code based on the CPU.


### What are `monitor_bpfle.o` and `monitor_bpfle.go` files?
- C to Bytecode: bpf2go calls a C compiler (Clang) to turn your .c file into monitor_bpfel.o. This is raw machine code for the eBPF virtual machine.
- The Embedding: bpf2go then creates monitor_bpfel.go. This file uses a special Go directive called //go:embed.
- The "Big Build": When you run go build, the Go compiler sees that //go:embed directive. It reaches out, grabs the binary data inside monitor_bpfel.o, and shoves it directly into your final Go executable.

### What is a Tracepoint?
- A Tracepoint is a static "hook" or marker manually placed by kernel developers at key locations in the Linux kernel source code (e.g., inside the network stack, scheduler, or memory management).

    - Purpose: To provide a stable way to observe kernel behavior without modifying or recompiling the kernel.

    - Stability: Unlike other methods, tracepoints are considered a stable API. They rarely change between kernel versions, making them perfect for production monitoring tools.

### How does a Tracepoint work?

- Tracepoints use a highly efficient mechanism to ensure they don't slow down the system when not in use.

    - Inactive State: The tracepoint is represented in the CPU instructions as a NOP (No-Operation). The CPU simply skips over it with near-zero performance hit.

    - Active State: When you "enable" the tracepoint, the kernel dynamically patches the code, replacing the NOP with a Jump instruction.

    - The Loop: The Jump sends the CPU to your tracing program (the probe), executes your logic, and then jumps back to the original kernel instruction.

### How to view tracepoint formats?

- Every tracepoint has a directory in the debugfs file system. You can view the specific data a tracepoint provides by reading its format file.
Command: ```bash sudo cat /sys/kernel/debug/tracing/events/skb/kfree_skb/format ```

### Why is the format file used?

- Defining offsets: It tells our program exactly how many bytes into the memory buffer a specific field starts
- Structuring eBPF: When writing eBPF code, you use this file to create a C struct that matches the kernel's data layout.
- Human Readability: It includes a print fmt line that shows how to turn the raw binary data into a readable string (e.g., converting an IP address from hex to a dot-decimal string).

### WHAT memory buffer?

- When a tracepoint fires, it doesn't just log the data, it takes a snapshot of raw data from the CPU's RAM and puts it into the RIng Buffer. 
- The kernel developers hardcoded these offsets, which is what is stored inside the format files. 
- Because the space is pre-determined, if your laptop is dropping 1 million packets per second and your eBPF program is trying to write an event for every single one, the ring buffer will eventually fill up. If the "Tail" of the buffer catches up to the "Head," the kernel will start dropping your trace data to avoid slowing down the actual network traffic.

### Tracepoints and Kprobes?
- Tracepoints: These are pre-installed "faucets" (hooks) put there by the builders. They are easy to use and won't leak, but you can only use them where they exist.
-Kprobes: These are like taking a drill and making your own hole (hook) anywhere in the pipe. You can hook a Kprobe to almost any function name in the kernel (tcp_v4_rcv, ip_output, etc.), even if there isn't an official tracepoint there.

### Why not use binary.Read()?

- It uses a feature called Reflection where it looks at the monitorEvent struct and tries to match every field one by one.
- But since we used bpf2go to generate monitorEvent struct, it contains a hidden field called HostLayout.
- binary.Read() doesn't know how many bytes that takes, so it stops. 
- It is safe but too dumb to handle the special memory alignment required by the Linux kernel.
- binary.Read creates a lot of temporary objects in memory. In a high-concurrency system this triggers the "Garbage Collector," which causes "Lag Spikes."
- So we use 'unsafe'.

### How does `unsafe` work?

- Instead of "parsing" the data, we "reinterpret" the memory. We tell the CPU that a specific block of bytes is already our struct.
- ```event := *(*monitorEvent)(unsafe.Pointer(&record.RawSample[0]))```
- `&record.RawSample[0]` - Address of the first byte in the buffer
- `unsafe.Pointer(...)` - Type erasure. Converts the Go pointer into a "Universal Pointer" that bypasses type-safety checks.
- `(*monitorEvent)(...)` - Type reinterpretation. Tells the compiler: "Treat this address as the start of a monitorEvent struct.
- `*` - Derefernce. Copies data from the memory address to a Go variable.

### IDEAS
- Instead of collecting each snapshot for each dropped packet in the buffer, I can group them together into a hash map grouped by PID, reason, COUNT
- Initially, the functions couldn't be identified.
